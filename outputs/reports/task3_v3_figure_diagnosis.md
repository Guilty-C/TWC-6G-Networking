# Task3 v3 Figure Pipeline Diagnosis

## 1. Overview
- The Task3 v3 figures are generated by `tools/generate_task3_figs.py`, which loads CSV dumps from `outputs/dumps/*.csv`, `outputs/dumps/task3_v3/**/*.csv`, `outputs/dumps/task3_v2/*.csv`, and `task3/outputs/dumps/**/*.csv`.【F:tools/generate_task3_figs.py†L98-L110】
- The script normalizes columns (time, QoE, queues, energy), picks per-algorithm scenarios, and emits 18 figures (`fig01`–`fig18`) under `outputs/figures/task3_v3`.【F:tools/generate_task3_figs.py†L13-L17】【F:tools/generate_task3_figs.py†L1187-L1206】
- In this checkout there are **no CSV dumps**, so runtime inspection of plots/debug CSVs is impossible; `load_all_dumps()` returns an empty dataframe and prints a warning.【F:tools/generate_task3_figs.py†L111-L117】【fc5571†L1-L4】

## 2. Global Checks
- **Data coverage**: No files are present in any of the configured dump folders, so the pipeline currently has zero rows to process.【fc5571†L1-L4】
- **Column consistency**: The normalizer expects `t`, `mos`/`sWER` (or inferable), `V`, optional `window`, `grid_density`, `seed`, `Q/queue/q_semantic`, `E/energy` or `P`+`slot_sec`, `snr_db`, and violation flags. Without dumps we cannot verify actual coverage.
- **File naming/metadata**: Scenario metadata (`V`, `window`, `grid_density`, `seed`) are extracted solely from filenames via regex, so any deviation (e.g., missing `V10`, `w50`, `g4`, `s1` tokens) will produce `NaN` scene keys and destabilize later grouping.【F:tools/generate_task3_figs.py†L40-L85】
- **QoE derivation**: If `mos` is absent, QoE becomes `5 - 4*sWER`; if both are missing, QoE is undefined. Energy is derived from `E`, `energy`, `energy_cum`, or `P*slot_sec`, which can change units if inputs mix joules vs. dBm-watt assumptions.【F:tools/generate_task3_figs.py†L67-L101】

## 3. Core Logic Audit
### normalize_columns
- Renames flexible time/QoE/queue/energy column names, backfills missing `t` with an integer index, and infers scenario metadata from filenames.【F:tools/generate_task3_figs.py†L40-L85】
- QoE falls back to `5-4*sWER`, assuming PESQ≈QoE scale compatibility; queue and energy are inferred from multiple aliases, and energy may be derived from power×slot length if cumulative energy is missing.【F:tools/generate_task3_figs.py†L67-L101】
- **Risks**: Misparsed filenames or missing metadata lead to `NaN` `V/window/grid_density/seed`, collapsing grouping; mixed units for energy/slot time or non-linear PESQ↔QoE mapping can distort metrics.

### load_all_dumps
- Globs four path patterns and skips files containing `summary/eval/report/phys_scan/resolved/config` in the basename.【F:tools/generate_task3_figs.py†L98-L117】
- Concatenates all normalized frames. Mixing task2/task3/task3_v2 paths can pool heterogeneous experiments without version tags.
- **Risks**: No guard to ensure only Task3 v3 experiments; silently ignores read failures; with heterogeneous sources, later per-figure filters may aggregate incomparable runs.

### select_best_V_scenario
- Splits an algorithm’s data into “vsweep” vs “non-vsweep” (filename/flags), then **picks the single (window, grid_density) combination with the widest V coverage**, preferring non-vsweep when coverage ties.【F:tools/generate_task3_figs.py†L119-L159】
- Only the chosen scene is plotted for that algorithm; other windows/grid densities are discarded.
- **Risks**: Algorithms are compared on different scenes; if the best-covered scene is small (few seeds/V values), QoE vs V curves become flat or sparse. Heterogeneous coverage across algorithms explains Fig02 flattening even when raw data vary.

### _steady_state_stats
- For each (algorithm, V, file, seed), computes `Tcommon` as the minimum horizon across seeds/files at that V, burns the first 50% of `Tcommon`, applies a rolling mean with window `max(25, 10% of remaining length)`, and averages across seeds with CI95.【F:tools/generate_task3_figs.py†L161-L237】
- **Risks**: Short runs or mismatched horizons shrink `Tcommon` and burn away most samples; aggressive smoothing can flatten trends; if only one seed exists, CI collapses to zero, hiding variability.

## 4. Per-Figure Diagnosis (Fig01–Fig18)
Each status reflects code/logic alignment; no runtime plots are available due to missing data.

1) **Fig01 Regret vs Time**
- Intended: Cumulative regret over time across algorithms.
- Implemented: Aligns mean QoE per integer `t` across algorithms, computes per-alg regret vs. best-at-each-t, then cumulative sum.【F:tools/generate_task3_figs.py†L239-L269】
- Status: ⚠️ Partial — uses QoE as reward without checking common horizon/arms; best-at-each-t may mix algorithms, overstating regret.

2) **Fig02 QoE vs V**
- Intended: Steady-state QoE trend across V for each algorithm.
- Implemented: Per-algorithm single-scene selection via `select_best_V_scenario`, then `_steady_state_stats` on QoE and plot mean±CI vs V.【F:tools/generate_task3_figs.py†L271-L332】
- Status: ⚠️ Partial — scene selection differs per algorithm; heavy burn-in/rolling can flatten curves. Likely source of “flat” QoE because differing V coverage and smoothing hide variation.

3) **Fig03 Queue/Energy vs V**
- Intended: Trade-off of average queue and energy across V.
- Implemented: Same scene selection as Fig02, separate stats for inferred queue/energy, twin y-axes.【F:tools/generate_task3_figs.py†L334-L421】
- Status: ⚠️ Partial — same scene-selection/smoothing issues; energy units may be inconsistent across sources.

4) **Fig04 Parameter Sensitivity**
- Intended: Sensitivity of QoE to parameters (window, grid density, epsilon, L_P, L_B).
- Implemented: Reads summary CSV or all data, infers params from filenames if missing, computes max–min mean QoE per parameter (no interaction, no control of algorithm/V).【F:tools/generate_task3_figs.py†L423-L504】
- Status: ❌ Conceptually weak — mixes algorithms and scenarios; deltas reflect cross-scene differences, not controlled sensitivity.

5) **Fig05 Nonstationary Robustness**
- Intended: Regret vs nonstationarity level.
- Implemented: For each file/seed, proxies nonstationarity via |ΔSNR|, drift_level, V_T, or S; regret approximated from regret_t, instant_regret, or QoE shortfall; plots regret vs proxy by algorithm.【F:tools/generate_task3_figs.py†L506-L576】
- Status: ⚠️ Partial — proxies arbitrary and mixed; regret computed inconsistently; bins applied per-alg without common scale.

6) **Fig06 Latency CDF**
- Intended: Decision latency distribution per algorithm.
- Implemented: First latency-like column, CDF per algorithm if ≥20 samples.【F:tools/generate_task3_figs.py†L578-L630】
- Status: ✅ Semantically fine, assuming latency column exists.

7) **Fig07 SNR–PER Heatmap**
- Intended: PER across bandwidth/SNR grid from phys_scan.
- Implemented: Uses `outputs/dumps/task3_phys_scan.csv`, pivots B vs snr_db.【F:tools/generate_task3_figs.py†L632-L676】
- Status: ⚠️ Partial — relies on external file not in repo; no validation of units/resolution.

8) **Fig08 P/B Selection Heatmap**
- Intended: Frequency of power-bandwidth choices.
- Implemented: Uses all P/B samples across algorithms, bins to 0.1 increments, counts occurrences.【F:tools/generate_task3_figs.py†L678-L724】
- Status: ⚠️ Partial — mixes all modes/seeds; no normalization by time or algorithm.

9) **Fig09 sWER vs SNR**
- Intended: Semantic error vs channel quality.
- Implemented: Per-algorithm binning of sWER by SNR (40 bins) and plotting means.【F:tools/generate_task3_figs.py†L726-L757】
- Status: ✅ Reasonable, though bin choice arbitrary.

10) **Fig10 QoE–sWER Frontier**
- Intended: Pareto frontier between QoE and error.
- Implemented: Mean QoE vs mean sWER per algorithm; frontier via cumulative max of sorted sWER.【F:tools/generate_task3_figs.py†L759-L789】
- Status: ⚠️ Partial — ignores V/scene differences; per-alg mean conflates regimes.

11) **Fig11 Violation over Time**
- Intended: Constraint violation rate over time.
- Implemented: Finds any violation flag columns, averages per integer t across all algorithms, smooths with long rolling window.【F:tools/generate_task3_figs.py†L791-L839】
- Status: ❌ Conceptually off — mixes algorithms/scenes; no separation by constraint type or algorithm.

12) **Fig12 Energy Efficiency Bar**
- Intended: QoE per energy by algorithm.
- Implemented: Requires summary CSV or derived energy+QoE; computes (qoe/energy).mean per algorithm, bars.【F:tools/generate_task3_figs.py†L841-L909】
- Status: ⚠️ Partial — energy source may be inconsistent; averaging ratios across heterogeneous runs is unstable.

13) **Fig13 QoE by Semantic Weight**
- Intended: QoE vs semantic importance buckets.
- Implemented: Quantile bins of `sem_weight` (or sWER fallback), mean QoE per bin per algorithm.【F:tools/generate_task3_figs.py†L911-L973】
- Status: ⚠️ Partial — mixes algorithms, no control of V/scene; sWER fallback changes meaning.

14) **Fig14 Ridgeline**
- Intended: Distribution of MOS across semantic bins.
- Implemented: KDE ridgeline across quantile bins of sem_weight or sWER, across all data.【F:tools/generate_task3_figs.py†L975-L1020】
- Status: ⚠️ Partial — cross-algorithm pooling and sWER fallback muddle semantic interpretation.

15) **Fig15 Semantic Constraint Curve**
- Intended: QoE under varying semantic violation thresholds.
- Implemented: Filters rows with sWER ≤ threshold (0.15–0.25), mean MOS; single curve, no algorithm separation.【F:tools/generate_task3_figs.py†L1022-L1045】
- Status: ❌ Conceptually wrong — ignores algorithms/V, uses MOS not QoE, narrow threshold range.

16) **Fig16 Recovery Time Bar**
- Intended: Time to recover QoE after a shock per algorithm.
- Implemented: Rolling mean MOS per file/seed, time to reach 90% of final steady level, average per algorithm.【F:tools/generate_task3_figs.py†L1047-L1088】
- Status: ⚠️ Partial — assumes final steady state and monotonic recovery; no event detection; MOS vs QoE inconsistency.

17) **Fig17 Window Scan**
- Intended: Regret vs sliding-window size.
- Implemented: Chooses preferred algorithm (`sw_ucb` if present), computes regret per file from regret_t/instant_regret or QoE shortfall, averages by window inferred from filename.【F:tools/generate_task3_figs.py†L1090-L1135】
- Status: ❌ Conceptually weak — single algorithm prioritized; regret proxy inconsistent; filename parsing fragile.

18) **Fig18 Grid Density vs Regret**
- Intended: Impact of grid density on regret.
- Implemented: Regret per file (same proxies as Fig17), averaged by grid_density from filename; plots mean regret vs grid density.【F:tools/generate_task3_figs.py†L1137-L1185】
- Status: ⚠️ Partial — mixes algorithms, regret proxy shaky, filename parsing sensitive.

## 5. Root Cause Analysis (prioritized)
1. **Per-algorithm scene cherry-picking** (`select_best_V_scenario`): Each algorithm uses whichever `(window, grid_density)` offers the widest V coverage, so Fig02/03 compare different environments and may pick sparse V sets, flattening curves. 【F:tools/generate_task3_figs.py†L119-L159】
2. **Aggressive steady-state trimming/smoothing**: `_steady_state_stats` burns 50% of the common horizon and applies long rolling means, which can eliminate early dynamics and shrink per-V samples, leading to flat QoE/queue/energy trends. 【F:tools/generate_task3_figs.py†L161-L237】
3. **Heterogeneous data pooling**: `load_all_dumps` mixes task2/task3/v2 outputs without tags, and many figures aggregate across algorithms or ignore V, causing semantically mismatched comparisons (e.g., Fig10, Fig11, Fig12, Fig13–15). 【F:tools/generate_task3_figs.py†L98-L117】【F:tools/generate_task3_figs.py†L759-L909】
4. **Filename-dependent metadata**: V/window/grid_density/seed are inferred solely from filenames; missing tokens yield NaNs that either drop rows or collapse grouping, distorting coverage and potentially excluding entire V points. 【F:tools/generate_task3_figs.py†L40-L85】
5. **Inconsistent metric derivations**: QoE from PESQ vs. `5-4*sWER`, energy from disparate sources, and regret approximations from QoE shortfall create unit/scale inconsistencies across figures (notably Fig03, Fig12, Fig17–18). 【F:tools/generate_task3_figs.py†L67-L101】【F:tools/generate_task3_figs.py†L334-L421】【F:tools/generate_task3_figs.py†L841-L909】【F:tools/generate_task3_figs.py†L1090-L1185】
6. **Cross-algorithm averaging without controls**: Several figures (e.g., Fig10, Fig11, Fig14–15) pool all algorithms and scenes, masking algorithm-specific behavior and mixing incompatible runs, leading to misleading “average” curves.

## 6. Suggested Fix Directions (high-level)
- **Unify scenes for V sweeps**: For Fig02/03, require a common `(window, grid_density)` across algorithms (or plot multiple panels) instead of per-algorithm best-scene selection; fall back to explicit config mapping when coverage differs.
- **Adjust steady-state logic**: Reduce burn-in fraction, expose smoothing window as a parameter, and drop the `Tcommon` clipping when runs already share seeds/horizons; report per-V sample counts to diagnose sparsity.
- **Tighten data filtering**: Restrict `load_all_dumps` to task3_v3-tagged files or require a version tag column; add assertions for required metadata/columns before plotting.
- **Explicit metadata ingestion**: Prefer metadata columns over filename parsing; if absent, reject files or log diagnostics so coverage issues are visible.
- **Metric hygiene**: Standardize QoE definition (PESQ vs. sWER-derived), fix energy units, and use consistent regret calculation tied to a reference policy rather than per-time best.
- **Algorithm-separated plots**: Avoid pooling across algorithms where semantics differ (Fig10–15); either facet by algorithm or normalize within controlled scenarios.

## 7. Conclusion
- The figure pipeline is structurally present but currently starved of data. The main semantic issues stem from per-algorithm scene selection, heavy smoothing, heterogeneous data pooling, and reliance on filename-derived metadata. Fig02’s flat QoE vs V is most plausibly caused by the combination of cherry-picked scenes with limited V coverage and aggressive steady-state smoothing rather than a plotting bug. Once consistent Task3 v3 dumps are available, applying the above fixes should yield more interpretable trends across V and other parameters.
